{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-DA2-19BCE1494-RishikGanta.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zRhjqD9yPkwi",
        "bz_Ir5AEQmRq",
        "Eo_oeNQ3RYKC",
        "UPCcbtN-Srhr",
        "q4N4vSHyUQHi",
        "la39dlnPV8PX",
        "Hi1zx6gjXW-D"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1>CSE4022 Natural Language Processing</h1></center>\n",
        "<center><h2>Digital Assignment -2</h2></center>"
      ],
      "metadata": {
        "id": "9hdPRzGK47xp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##1.\tCreate a text corpus with minimum 200 words (unique contents). Implement the following text processing(10 Marks)\n",
        "* Word segmentation\n",
        "* Sentence segmentation\n",
        "* Convert to Lowercase\n",
        "* Stop words removal\n",
        "* Stemming\n",
        "* Lemmatization\n",
        "* Part of speech tagger \n"
      ],
      "metadata": {
        "id": "HlagbV9hNItb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Rishik Ganta\n",
        "#### 19BCE1494"
      ],
      "metadata": {
        "id": "BuUpxbmKOXLG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "cQ0AIZAMM9_Q",
        "outputId": "413b2dc1-92a3-4d0e-a664-14063e7ae2c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"As she sat watching the world go by, something caught her eye. It wasn't so much its color or shape, but the way it was moving. She squinted to see if she could better understand what it was and where it was going, but it didn't help. As she continued to stare into the distance, she didn't understand why this uneasiness was building inside her body. She felt like she should get up and run. If only she could make out what it was. At that moment, she comprehended what it was and where it was heading, and she knew her life would never be the same.It all started with a random letter. Several of those were joined forces to create a random word. The words decided to get together and form a random sentence. They decided not to stop there and it wasn't long before a random paragraph had been cobbled together. The question was whether or not they could continue the momentum long enough to create a random short story.There weren't supposed to be dragons flying in the sky. First and foremost, dragons didn't exist. They were mythical creatures from fantasy books like unicorns. This was something that Pete knew in his heart to be true so he was having a difficult time acknowledging that there were actually fire-breathing dragons flying in the sky above him.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "corpus = \"As she sat watching the world go by, something caught her eye. It wasn't so much its color or shape, but the way it was moving. She squinted to see if she could better understand what it was and where it was going, but it didn't help. As she continued to stare into the distance, she didn't understand why this uneasiness was building inside her body. She felt like she should get up and run. If only she could make out what it was. At that moment, she comprehended what it was and where it was heading, and she knew her life would never be the same.It all started with a random letter. Several of those were joined forces to create a random word. The words decided to get together and form a random sentence. They decided not to stop there and it wasn't long before a random paragraph had been cobbled together. The question was whether or not they could continue the momentum long enough to create a random short story.There weren't supposed to be dragons flying in the sky. First and foremost, dragons didn't exist. They were mythical creatures from fantasy books like unicorns. This was something that Pete knew in his heart to be true so he was having a difficult time acknowledging that there were actually fire-breathing dragons flying in the sky above him.\"\n",
        "corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word segmentation"
      ],
      "metadata": {
        "id": "zRhjqD9yPkwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1hfGHnSQY5n",
        "outputId": "041a69ae-0682-4906-d0ba-51b1d64ff6e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "words = word_tokenize(corpus)"
      ],
      "metadata": {
        "id": "i2pWF_avPkTc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NBrgBQzPbqY",
        "outputId": "953ab502-b985-448c-e422-76b151493dbe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['As',\n",
              " 'she',\n",
              " 'sat',\n",
              " 'watching',\n",
              " 'the',\n",
              " 'world',\n",
              " 'go',\n",
              " 'by',\n",
              " ',',\n",
              " 'something',\n",
              " 'caught',\n",
              " 'her',\n",
              " 'eye',\n",
              " '.',\n",
              " 'It',\n",
              " 'was',\n",
              " \"n't\",\n",
              " 'so',\n",
              " 'much',\n",
              " 'its',\n",
              " 'color',\n",
              " 'or',\n",
              " 'shape',\n",
              " ',',\n",
              " 'but',\n",
              " 'the',\n",
              " 'way',\n",
              " 'it',\n",
              " 'was',\n",
              " 'moving',\n",
              " '.',\n",
              " 'She',\n",
              " 'squinted',\n",
              " 'to',\n",
              " 'see',\n",
              " 'if',\n",
              " 'she',\n",
              " 'could',\n",
              " 'better',\n",
              " 'understand',\n",
              " 'what',\n",
              " 'it',\n",
              " 'was',\n",
              " 'and',\n",
              " 'where',\n",
              " 'it',\n",
              " 'was',\n",
              " 'going',\n",
              " ',',\n",
              " 'but',\n",
              " 'it',\n",
              " 'did',\n",
              " \"n't\",\n",
              " 'help',\n",
              " '.',\n",
              " 'As',\n",
              " 'she',\n",
              " 'continued',\n",
              " 'to',\n",
              " 'stare',\n",
              " 'into',\n",
              " 'the',\n",
              " 'distance',\n",
              " ',',\n",
              " 'she',\n",
              " 'did',\n",
              " \"n't\",\n",
              " 'understand',\n",
              " 'why',\n",
              " 'this',\n",
              " 'uneasiness',\n",
              " 'was',\n",
              " 'building',\n",
              " 'inside',\n",
              " 'her',\n",
              " 'body',\n",
              " '.',\n",
              " 'She',\n",
              " 'felt',\n",
              " 'like',\n",
              " 'she',\n",
              " 'should',\n",
              " 'get',\n",
              " 'up',\n",
              " 'and',\n",
              " 'run',\n",
              " '.',\n",
              " 'If',\n",
              " 'only',\n",
              " 'she',\n",
              " 'could',\n",
              " 'make',\n",
              " 'out',\n",
              " 'what',\n",
              " 'it',\n",
              " 'was',\n",
              " '.',\n",
              " 'At',\n",
              " 'that',\n",
              " 'moment',\n",
              " ',',\n",
              " 'she',\n",
              " 'comprehended',\n",
              " 'what',\n",
              " 'it',\n",
              " 'was',\n",
              " 'and',\n",
              " 'where',\n",
              " 'it',\n",
              " 'was',\n",
              " 'heading',\n",
              " ',',\n",
              " 'and',\n",
              " 'she',\n",
              " 'knew',\n",
              " 'her',\n",
              " 'life',\n",
              " 'would',\n",
              " 'never',\n",
              " 'be',\n",
              " 'the',\n",
              " 'same.It',\n",
              " 'all',\n",
              " 'started',\n",
              " 'with',\n",
              " 'a',\n",
              " 'random',\n",
              " 'letter',\n",
              " '.',\n",
              " 'Several',\n",
              " 'of',\n",
              " 'those',\n",
              " 'were',\n",
              " 'joined',\n",
              " 'forces',\n",
              " 'to',\n",
              " 'create',\n",
              " 'a',\n",
              " 'random',\n",
              " 'word',\n",
              " '.',\n",
              " 'The',\n",
              " 'words',\n",
              " 'decided',\n",
              " 'to',\n",
              " 'get',\n",
              " 'together',\n",
              " 'and',\n",
              " 'form',\n",
              " 'a',\n",
              " 'random',\n",
              " 'sentence',\n",
              " '.',\n",
              " 'They',\n",
              " 'decided',\n",
              " 'not',\n",
              " 'to',\n",
              " 'stop',\n",
              " 'there',\n",
              " 'and',\n",
              " 'it',\n",
              " 'was',\n",
              " \"n't\",\n",
              " 'long',\n",
              " 'before',\n",
              " 'a',\n",
              " 'random',\n",
              " 'paragraph',\n",
              " 'had',\n",
              " 'been',\n",
              " 'cobbled',\n",
              " 'together',\n",
              " '.',\n",
              " 'The',\n",
              " 'question',\n",
              " 'was',\n",
              " 'whether',\n",
              " 'or',\n",
              " 'not',\n",
              " 'they',\n",
              " 'could',\n",
              " 'continue',\n",
              " 'the',\n",
              " 'momentum',\n",
              " 'long',\n",
              " 'enough',\n",
              " 'to',\n",
              " 'create',\n",
              " 'a',\n",
              " 'random',\n",
              " 'short',\n",
              " 'story.There',\n",
              " 'were',\n",
              " \"n't\",\n",
              " 'supposed',\n",
              " 'to',\n",
              " 'be',\n",
              " 'dragons',\n",
              " 'flying',\n",
              " 'in',\n",
              " 'the',\n",
              " 'sky',\n",
              " '.',\n",
              " 'First',\n",
              " 'and',\n",
              " 'foremost',\n",
              " ',',\n",
              " 'dragons',\n",
              " 'did',\n",
              " \"n't\",\n",
              " 'exist',\n",
              " '.',\n",
              " 'They',\n",
              " 'were',\n",
              " 'mythical',\n",
              " 'creatures',\n",
              " 'from',\n",
              " 'fantasy',\n",
              " 'books',\n",
              " 'like',\n",
              " 'unicorns',\n",
              " '.',\n",
              " 'This',\n",
              " 'was',\n",
              " 'something',\n",
              " 'that',\n",
              " 'Pete',\n",
              " 'knew',\n",
              " 'in',\n",
              " 'his',\n",
              " 'heart',\n",
              " 'to',\n",
              " 'be',\n",
              " 'true',\n",
              " 'so',\n",
              " 'he',\n",
              " 'was',\n",
              " 'having',\n",
              " 'a',\n",
              " 'difficult',\n",
              " 'time',\n",
              " 'acknowledging',\n",
              " 'that',\n",
              " 'there',\n",
              " 'were',\n",
              " 'actually',\n",
              " 'fire-breathing',\n",
              " 'dragons',\n",
              " 'flying',\n",
              " 'in',\n",
              " 'the',\n",
              " 'sky',\n",
              " 'above',\n",
              " 'him',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence segmentation"
      ],
      "metadata": {
        "id": "bz_Ir5AEQmRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sentences=sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "Lzhu5tWnQl0n"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtQsDtb1Qip4",
        "outputId": "38fae024-ec52-4e85-abef-5f37a619268a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['As she sat watching the world go by, something caught her eye.',\n",
              " \"It wasn't so much its color or shape, but the way it was moving.\",\n",
              " \"She squinted to see if she could better understand what it was and where it was going, but it didn't help.\",\n",
              " \"As she continued to stare into the distance, she didn't understand why this uneasiness was building inside her body.\",\n",
              " 'She felt like she should get up and run.',\n",
              " 'If only she could make out what it was.',\n",
              " 'At that moment, she comprehended what it was and where it was heading, and she knew her life would never be the same.It all started with a random letter.',\n",
              " 'Several of those were joined forces to create a random word.',\n",
              " 'The words decided to get together and form a random sentence.',\n",
              " \"They decided not to stop there and it wasn't long before a random paragraph had been cobbled together.\",\n",
              " \"The question was whether or not they could continue the momentum long enough to create a random short story.There weren't supposed to be dragons flying in the sky.\",\n",
              " \"First and foremost, dragons didn't exist.\",\n",
              " 'They were mythical creatures from fantasy books like unicorns.',\n",
              " 'This was something that Pete knew in his heart to be true so he was having a difficult time acknowledging that there were actually fire-breathing dragons flying in the sky above him.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert to lowercase"
      ],
      "metadata": {
        "id": "Eo_oeNQ3RYKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lower_case = [i.lower() for i in words]"
      ],
      "metadata": {
        "id": "45ljevbYRDo5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(lower_case)):\n",
        "  print(words[i], '--)',lower_case[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6g-1rRrR33h",
        "outputId": "e0443072-9e7e-49c6-9a96-ab3af9c56655"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As --) as\n",
            "she --) she\n",
            "sat --) sat\n",
            "watching --) watching\n",
            "the --) the\n",
            "world --) world\n",
            "go --) go\n",
            "by --) by\n",
            ", --) ,\n",
            "something --) something\n",
            "caught --) caught\n",
            "her --) her\n",
            "eye --) eye\n",
            ". --) .\n",
            "It --) it\n",
            "was --) was\n",
            "n't --) n't\n",
            "so --) so\n",
            "much --) much\n",
            "its --) its\n",
            "color --) color\n",
            "or --) or\n",
            "shape --) shape\n",
            ", --) ,\n",
            "but --) but\n",
            "the --) the\n",
            "way --) way\n",
            "it --) it\n",
            "was --) was\n",
            "moving --) moving\n",
            ". --) .\n",
            "She --) she\n",
            "squinted --) squinted\n",
            "to --) to\n",
            "see --) see\n",
            "if --) if\n",
            "she --) she\n",
            "could --) could\n",
            "better --) better\n",
            "understand --) understand\n",
            "what --) what\n",
            "it --) it\n",
            "was --) was\n",
            "and --) and\n",
            "where --) where\n",
            "it --) it\n",
            "was --) was\n",
            "going --) going\n",
            ", --) ,\n",
            "but --) but\n",
            "it --) it\n",
            "did --) did\n",
            "n't --) n't\n",
            "help --) help\n",
            ". --) .\n",
            "As --) as\n",
            "she --) she\n",
            "continued --) continued\n",
            "to --) to\n",
            "stare --) stare\n",
            "into --) into\n",
            "the --) the\n",
            "distance --) distance\n",
            ", --) ,\n",
            "she --) she\n",
            "did --) did\n",
            "n't --) n't\n",
            "understand --) understand\n",
            "why --) why\n",
            "this --) this\n",
            "uneasiness --) uneasiness\n",
            "was --) was\n",
            "building --) building\n",
            "inside --) inside\n",
            "her --) her\n",
            "body --) body\n",
            ". --) .\n",
            "She --) she\n",
            "felt --) felt\n",
            "like --) like\n",
            "she --) she\n",
            "should --) should\n",
            "get --) get\n",
            "up --) up\n",
            "and --) and\n",
            "run --) run\n",
            ". --) .\n",
            "If --) if\n",
            "only --) only\n",
            "she --) she\n",
            "could --) could\n",
            "make --) make\n",
            "out --) out\n",
            "what --) what\n",
            "it --) it\n",
            "was --) was\n",
            ". --) .\n",
            "At --) at\n",
            "that --) that\n",
            "moment --) moment\n",
            ", --) ,\n",
            "she --) she\n",
            "comprehended --) comprehended\n",
            "what --) what\n",
            "it --) it\n",
            "was --) was\n",
            "and --) and\n",
            "where --) where\n",
            "it --) it\n",
            "was --) was\n",
            "heading --) heading\n",
            ", --) ,\n",
            "and --) and\n",
            "she --) she\n",
            "knew --) knew\n",
            "her --) her\n",
            "life --) life\n",
            "would --) would\n",
            "never --) never\n",
            "be --) be\n",
            "the --) the\n",
            "same.It --) same.it\n",
            "all --) all\n",
            "started --) started\n",
            "with --) with\n",
            "a --) a\n",
            "random --) random\n",
            "letter --) letter\n",
            ". --) .\n",
            "Several --) several\n",
            "of --) of\n",
            "those --) those\n",
            "were --) were\n",
            "joined --) joined\n",
            "forces --) forces\n",
            "to --) to\n",
            "create --) create\n",
            "a --) a\n",
            "random --) random\n",
            "word --) word\n",
            ". --) .\n",
            "The --) the\n",
            "words --) words\n",
            "decided --) decided\n",
            "to --) to\n",
            "get --) get\n",
            "together --) together\n",
            "and --) and\n",
            "form --) form\n",
            "a --) a\n",
            "random --) random\n",
            "sentence --) sentence\n",
            ". --) .\n",
            "They --) they\n",
            "decided --) decided\n",
            "not --) not\n",
            "to --) to\n",
            "stop --) stop\n",
            "there --) there\n",
            "and --) and\n",
            "it --) it\n",
            "was --) was\n",
            "n't --) n't\n",
            "long --) long\n",
            "before --) before\n",
            "a --) a\n",
            "random --) random\n",
            "paragraph --) paragraph\n",
            "had --) had\n",
            "been --) been\n",
            "cobbled --) cobbled\n",
            "together --) together\n",
            ". --) .\n",
            "The --) the\n",
            "question --) question\n",
            "was --) was\n",
            "whether --) whether\n",
            "or --) or\n",
            "not --) not\n",
            "they --) they\n",
            "could --) could\n",
            "continue --) continue\n",
            "the --) the\n",
            "momentum --) momentum\n",
            "long --) long\n",
            "enough --) enough\n",
            "to --) to\n",
            "create --) create\n",
            "a --) a\n",
            "random --) random\n",
            "short --) short\n",
            "story.There --) story.there\n",
            "were --) were\n",
            "n't --) n't\n",
            "supposed --) supposed\n",
            "to --) to\n",
            "be --) be\n",
            "dragons --) dragons\n",
            "flying --) flying\n",
            "in --) in\n",
            "the --) the\n",
            "sky --) sky\n",
            ". --) .\n",
            "First --) first\n",
            "and --) and\n",
            "foremost --) foremost\n",
            ", --) ,\n",
            "dragons --) dragons\n",
            "did --) did\n",
            "n't --) n't\n",
            "exist --) exist\n",
            ". --) .\n",
            "They --) they\n",
            "were --) were\n",
            "mythical --) mythical\n",
            "creatures --) creatures\n",
            "from --) from\n",
            "fantasy --) fantasy\n",
            "books --) books\n",
            "like --) like\n",
            "unicorns --) unicorns\n",
            ". --) .\n",
            "This --) this\n",
            "was --) was\n",
            "something --) something\n",
            "that --) that\n",
            "Pete --) pete\n",
            "knew --) knew\n",
            "in --) in\n",
            "his --) his\n",
            "heart --) heart\n",
            "to --) to\n",
            "be --) be\n",
            "true --) true\n",
            "so --) so\n",
            "he --) he\n",
            "was --) was\n",
            "having --) having\n",
            "a --) a\n",
            "difficult --) difficult\n",
            "time --) time\n",
            "acknowledging --) acknowledging\n",
            "that --) that\n",
            "there --) there\n",
            "were --) were\n",
            "actually --) actually\n",
            "fire-breathing --) fire-breathing\n",
            "dragons --) dragons\n",
            "flying --) flying\n",
            "in --) in\n",
            "the --) the\n",
            "sky --) sky\n",
            "above --) above\n",
            "him --) him\n",
            ". --) .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop words removal"
      ],
      "metadata": {
        "id": "UPCcbtN-Srhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopword=stopwords.words('english')\n",
        "filtered_words = [word for word in lower_case if word not in stopword]"
      ],
      "metadata": {
        "id": "tN1SW6kUR453"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhO3hgSJTu_d",
        "outputId": "c731f5a3-d922-495b-b736-8e2ef45cd7d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sat',\n",
              " 'watching',\n",
              " 'world',\n",
              " 'go',\n",
              " ',',\n",
              " 'something',\n",
              " 'caught',\n",
              " 'eye',\n",
              " '.',\n",
              " \"n't\",\n",
              " 'much',\n",
              " 'color',\n",
              " 'shape',\n",
              " ',',\n",
              " 'way',\n",
              " 'moving',\n",
              " '.',\n",
              " 'squinted',\n",
              " 'see',\n",
              " 'could',\n",
              " 'better',\n",
              " 'understand',\n",
              " 'going',\n",
              " ',',\n",
              " \"n't\",\n",
              " 'help',\n",
              " '.',\n",
              " 'continued',\n",
              " 'stare',\n",
              " 'distance',\n",
              " ',',\n",
              " \"n't\",\n",
              " 'understand',\n",
              " 'uneasiness',\n",
              " 'building',\n",
              " 'inside',\n",
              " 'body',\n",
              " '.',\n",
              " 'felt',\n",
              " 'like',\n",
              " 'get',\n",
              " 'run',\n",
              " '.',\n",
              " 'could',\n",
              " 'make',\n",
              " '.',\n",
              " 'moment',\n",
              " ',',\n",
              " 'comprehended',\n",
              " 'heading',\n",
              " ',',\n",
              " 'knew',\n",
              " 'life',\n",
              " 'would',\n",
              " 'never',\n",
              " 'same.it',\n",
              " 'started',\n",
              " 'random',\n",
              " 'letter',\n",
              " '.',\n",
              " 'several',\n",
              " 'joined',\n",
              " 'forces',\n",
              " 'create',\n",
              " 'random',\n",
              " 'word',\n",
              " '.',\n",
              " 'words',\n",
              " 'decided',\n",
              " 'get',\n",
              " 'together',\n",
              " 'form',\n",
              " 'random',\n",
              " 'sentence',\n",
              " '.',\n",
              " 'decided',\n",
              " 'stop',\n",
              " \"n't\",\n",
              " 'long',\n",
              " 'random',\n",
              " 'paragraph',\n",
              " 'cobbled',\n",
              " 'together',\n",
              " '.',\n",
              " 'question',\n",
              " 'whether',\n",
              " 'could',\n",
              " 'continue',\n",
              " 'momentum',\n",
              " 'long',\n",
              " 'enough',\n",
              " 'create',\n",
              " 'random',\n",
              " 'short',\n",
              " 'story.there',\n",
              " \"n't\",\n",
              " 'supposed',\n",
              " 'dragons',\n",
              " 'flying',\n",
              " 'sky',\n",
              " '.',\n",
              " 'first',\n",
              " 'foremost',\n",
              " ',',\n",
              " 'dragons',\n",
              " \"n't\",\n",
              " 'exist',\n",
              " '.',\n",
              " 'mythical',\n",
              " 'creatures',\n",
              " 'fantasy',\n",
              " 'books',\n",
              " 'like',\n",
              " 'unicorns',\n",
              " '.',\n",
              " 'something',\n",
              " 'pete',\n",
              " 'knew',\n",
              " 'heart',\n",
              " 'true',\n",
              " 'difficult',\n",
              " 'time',\n",
              " 'acknowledging',\n",
              " 'actually',\n",
              " 'fire-breathing',\n",
              " 'dragons',\n",
              " 'flying',\n",
              " 'sky',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of tokens before stop word removal:\",len(lower_case))\n",
        "print(\"Number of tokens after stop word removal:\",len(filtered_words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1deD9HZBT_Dp",
        "outputId": "ed0cde99-aeb9-4837-a746-65fcb0c67ba7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens before stop word removal: 255\n",
            "Number of tokens after stop word removal: 129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemmming"
      ],
      "metadata": {
        "id": "q4N4vSHyUQHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "stemmed_words=[]\n",
        "for w in filtered_words:\n",
        "    stemmed_words.append(ps.stem(w))"
      ],
      "metadata": {
        "id": "tG17tT1rUT3p"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"word\",\"   \",\"stem\\n\")\n",
        "for i in range(len(filtered_words)):\n",
        "  print(filtered_words[i],'--)',stemmed_words[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa66tMc9VZ6m",
        "outputId": "48fd00c2-8658-4244-cdd3-b3dd8a4c68a2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word     stem\n",
            "\n",
            "sat --) sat\n",
            "watching --) watch\n",
            "world --) world\n",
            "go --) go\n",
            ", --) ,\n",
            "something --) someth\n",
            "caught --) caught\n",
            "eye --) eye\n",
            ". --) .\n",
            "n't --) n't\n",
            "much --) much\n",
            "color --) color\n",
            "shape --) shape\n",
            ", --) ,\n",
            "way --) way\n",
            "moving --) move\n",
            ". --) .\n",
            "squinted --) squint\n",
            "see --) see\n",
            "could --) could\n",
            "better --) better\n",
            "understand --) understand\n",
            "going --) go\n",
            ", --) ,\n",
            "n't --) n't\n",
            "help --) help\n",
            ". --) .\n",
            "continued --) continu\n",
            "stare --) stare\n",
            "distance --) distanc\n",
            ", --) ,\n",
            "n't --) n't\n",
            "understand --) understand\n",
            "uneasiness --) uneasi\n",
            "building --) build\n",
            "inside --) insid\n",
            "body --) bodi\n",
            ". --) .\n",
            "felt --) felt\n",
            "like --) like\n",
            "get --) get\n",
            "run --) run\n",
            ". --) .\n",
            "could --) could\n",
            "make --) make\n",
            ". --) .\n",
            "moment --) moment\n",
            ", --) ,\n",
            "comprehended --) comprehend\n",
            "heading --) head\n",
            ", --) ,\n",
            "knew --) knew\n",
            "life --) life\n",
            "would --) would\n",
            "never --) never\n",
            "same.it --) same.it\n",
            "started --) start\n",
            "random --) random\n",
            "letter --) letter\n",
            ". --) .\n",
            "several --) sever\n",
            "joined --) join\n",
            "forces --) forc\n",
            "create --) creat\n",
            "random --) random\n",
            "word --) word\n",
            ". --) .\n",
            "words --) word\n",
            "decided --) decid\n",
            "get --) get\n",
            "together --) togeth\n",
            "form --) form\n",
            "random --) random\n",
            "sentence --) sentenc\n",
            ". --) .\n",
            "decided --) decid\n",
            "stop --) stop\n",
            "n't --) n't\n",
            "long --) long\n",
            "random --) random\n",
            "paragraph --) paragraph\n",
            "cobbled --) cobbl\n",
            "together --) togeth\n",
            ". --) .\n",
            "question --) question\n",
            "whether --) whether\n",
            "could --) could\n",
            "continue --) continu\n",
            "momentum --) momentum\n",
            "long --) long\n",
            "enough --) enough\n",
            "create --) creat\n",
            "random --) random\n",
            "short --) short\n",
            "story.there --) story.ther\n",
            "n't --) n't\n",
            "supposed --) suppos\n",
            "dragons --) dragon\n",
            "flying --) fli\n",
            "sky --) sky\n",
            ". --) .\n",
            "first --) first\n",
            "foremost --) foremost\n",
            ", --) ,\n",
            "dragons --) dragon\n",
            "n't --) n't\n",
            "exist --) exist\n",
            ". --) .\n",
            "mythical --) mythic\n",
            "creatures --) creatur\n",
            "fantasy --) fantasi\n",
            "books --) book\n",
            "like --) like\n",
            "unicorns --) unicorn\n",
            ". --) .\n",
            "something --) someth\n",
            "pete --) pete\n",
            "knew --) knew\n",
            "heart --) heart\n",
            "true --) true\n",
            "difficult --) difficult\n",
            "time --) time\n",
            "acknowledging --) acknowledg\n",
            "actually --) actual\n",
            "fire-breathing --) fire-breath\n",
            "dragons --) dragon\n",
            "flying --) fli\n",
            "sky --) sky\n",
            ". --) .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization"
      ],
      "metadata": {
        "id": "la39dlnPV8PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import \tWordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemma_words=[]\n",
        "for w in filtered_words:\n",
        "\t\tlemma_words.append(lemmatizer.lemmatize(w))"
      ],
      "metadata": {
        "id": "_Uu21LcKVdRI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"word\",\"   \",\"lemma\\n\")\n",
        "for i in range(len(filtered_words)):\n",
        "  print(filtered_words[i],'--)',lemma_words[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qasn9paUW69f",
        "outputId": "1cf92715-ebac-478f-b8d3-09c03bf55e6b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word     lemma\n",
            "\n",
            "sat --) sat\n",
            "watching --) watching\n",
            "world --) world\n",
            "go --) go\n",
            ", --) ,\n",
            "something --) something\n",
            "caught --) caught\n",
            "eye --) eye\n",
            ". --) .\n",
            "n't --) n't\n",
            "much --) much\n",
            "color --) color\n",
            "shape --) shape\n",
            ", --) ,\n",
            "way --) way\n",
            "moving --) moving\n",
            ". --) .\n",
            "squinted --) squinted\n",
            "see --) see\n",
            "could --) could\n",
            "better --) better\n",
            "understand --) understand\n",
            "going --) going\n",
            ", --) ,\n",
            "n't --) n't\n",
            "help --) help\n",
            ". --) .\n",
            "continued --) continued\n",
            "stare --) stare\n",
            "distance --) distance\n",
            ", --) ,\n",
            "n't --) n't\n",
            "understand --) understand\n",
            "uneasiness --) uneasiness\n",
            "building --) building\n",
            "inside --) inside\n",
            "body --) body\n",
            ". --) .\n",
            "felt --) felt\n",
            "like --) like\n",
            "get --) get\n",
            "run --) run\n",
            ". --) .\n",
            "could --) could\n",
            "make --) make\n",
            ". --) .\n",
            "moment --) moment\n",
            ", --) ,\n",
            "comprehended --) comprehended\n",
            "heading --) heading\n",
            ", --) ,\n",
            "knew --) knew\n",
            "life --) life\n",
            "would --) would\n",
            "never --) never\n",
            "same.it --) same.it\n",
            "started --) started\n",
            "random --) random\n",
            "letter --) letter\n",
            ". --) .\n",
            "several --) several\n",
            "joined --) joined\n",
            "forces --) force\n",
            "create --) create\n",
            "random --) random\n",
            "word --) word\n",
            ". --) .\n",
            "words --) word\n",
            "decided --) decided\n",
            "get --) get\n",
            "together --) together\n",
            "form --) form\n",
            "random --) random\n",
            "sentence --) sentence\n",
            ". --) .\n",
            "decided --) decided\n",
            "stop --) stop\n",
            "n't --) n't\n",
            "long --) long\n",
            "random --) random\n",
            "paragraph --) paragraph\n",
            "cobbled --) cobbled\n",
            "together --) together\n",
            ". --) .\n",
            "question --) question\n",
            "whether --) whether\n",
            "could --) could\n",
            "continue --) continue\n",
            "momentum --) momentum\n",
            "long --) long\n",
            "enough --) enough\n",
            "create --) create\n",
            "random --) random\n",
            "short --) short\n",
            "story.there --) story.there\n",
            "n't --) n't\n",
            "supposed --) supposed\n",
            "dragons --) dragon\n",
            "flying --) flying\n",
            "sky --) sky\n",
            ". --) .\n",
            "first --) first\n",
            "foremost --) foremost\n",
            ", --) ,\n",
            "dragons --) dragon\n",
            "n't --) n't\n",
            "exist --) exist\n",
            ". --) .\n",
            "mythical --) mythical\n",
            "creatures --) creature\n",
            "fantasy --) fantasy\n",
            "books --) book\n",
            "like --) like\n",
            "unicorns --) unicorn\n",
            ". --) .\n",
            "something --) something\n",
            "pete --) pete\n",
            "knew --) knew\n",
            "heart --) heart\n",
            "true --) true\n",
            "difficult --) difficult\n",
            "time --) time\n",
            "acknowledging --) acknowledging\n",
            "actually --) actually\n",
            "fire-breathing --) fire-breathing\n",
            "dragons --) dragon\n",
            "flying --) flying\n",
            "sky --) sky\n",
            ". --) .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part of speech"
      ],
      "metadata": {
        "id": "128gELvmX-8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "pos_tag(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDCB6czaXWFe",
        "outputId": "ef009482-5552-42f9-8103-99a83c46ee77"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('As', 'IN'),\n",
              " ('she', 'PRP'),\n",
              " ('sat', 'VBD'),\n",
              " ('watching', 'VBG'),\n",
              " ('the', 'DT'),\n",
              " ('world', 'NN'),\n",
              " ('go', 'NN'),\n",
              " ('by', 'IN'),\n",
              " (',', ','),\n",
              " ('something', 'NN'),\n",
              " ('caught', 'VBD'),\n",
              " ('her', 'PRP$'),\n",
              " ('eye', 'NN'),\n",
              " ('.', '.'),\n",
              " ('It', 'PRP'),\n",
              " ('was', 'VBD'),\n",
              " (\"n't\", 'RB'),\n",
              " ('so', 'RB'),\n",
              " ('much', 'JJ'),\n",
              " ('its', 'PRP$'),\n",
              " ('color', 'NN'),\n",
              " ('or', 'CC'),\n",
              " ('shape', 'NN'),\n",
              " (',', ','),\n",
              " ('but', 'CC'),\n",
              " ('the', 'DT'),\n",
              " ('way', 'NN'),\n",
              " ('it', 'PRP'),\n",
              " ('was', 'VBD'),\n",
              " ('moving', 'VBG'),\n",
              " ('.', '.'),\n",
              " ('She', 'PRP'),\n",
              " ('squinted', 'VBD'),\n",
              " ('to', 'TO'),\n",
              " ('see', 'VB'),\n",
              " ('if', 'IN'),\n",
              " ('she', 'PRP'),\n",
              " ('could', 'MD'),\n",
              " ('better', 'RB'),\n",
              " ('understand', 'VB'),\n",
              " ('what', 'WP'),\n",
              " ('it', 'PRP'),\n",
              " ('was', 'VBD'),\n",
              " ('and', 'CC'),\n",
              " ('where', 'WRB'),\n",
              " ('it', 'PRP'),\n",
              " ('was', 'VBD'),\n",
              " ('going', 'VBG'),\n",
              " (',', ','),\n",
              " ('but', 'CC'),\n",
              " ('it', 'PRP'),\n",
              " ('did', 'VBD'),\n",
              " (\"n't\", 'RB'),\n",
              " ('help', 'VB'),\n",
              " ('.', '.'),\n",
              " ('As', 'IN'),\n",
              " ('she', 'PRP'),\n",
              " ('continued', 'VBD'),\n",
              " ('to', 'TO'),\n",
              " ('stare', 'VB'),\n",
              " ('into', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('distance', 'NN'),\n",
              " (',', ','),\n",
              " ('she', 'PRP'),\n",
              " ('did', 'VBD'),\n",
              " (\"n't\", 'RB'),\n",
              " ('understand', 'VB'),\n",
              " ('why', 'WRB'),\n",
              " ('this', 'DT'),\n",
              " ('uneasiness', 'NN'),\n",
              " ('was', 'VBD'),\n",
              " ('building', 'VBG'),\n",
              " ('inside', 'IN'),\n",
              " ('her', 'PRP$'),\n",
              " ('body', 'NN'),\n",
              " ('.', '.'),\n",
              " ('She', 'PRP'),\n",
              " ('felt', 'VBD'),\n",
              " ('like', 'IN'),\n",
              " ('she', 'PRP'),\n",
              " ('should', 'MD'),\n",
              " ('get', 'VB'),\n",
              " ('up', 'RP'),\n",
              " ('and', 'CC'),\n",
              " ('run', 'VB'),\n",
              " ('.', '.'),\n",
              " ('If', 'IN'),\n",
              " ('only', 'RB'),\n",
              " ('she', 'PRP'),\n",
              " ('could', 'MD'),\n",
              " ('make', 'VB'),\n",
              " ('out', 'RP'),\n",
              " ('what', 'WP'),\n",
              " ('it', 'PRP'),\n",
              " ('was', 'VBD'),\n",
              " ('.', '.'),\n",
              " ('At', 'IN'),\n",
              " ('that', 'DT'),\n",
              " ('moment', 'NN'),\n",
              " (',', ','),\n",
              " ('she', 'PRP'),\n",
              " ('comprehended', 'VBD'),\n",
              " ('what', 'WP'),\n",
              " ('it', 'PRP'),\n",
              " ('was', 'VBD'),\n",
              " ('and', 'CC'),\n",
              " ('where', 'WRB'),\n",
              " ('it', 'PRP'),\n",
              " ('was', 'VBD'),\n",
              " ('heading', 'VBG'),\n",
              " (',', ','),\n",
              " ('and', 'CC'),\n",
              " ('she', 'PRP'),\n",
              " ('knew', 'VBD'),\n",
              " ('her', 'PRP'),\n",
              " ('life', 'NN'),\n",
              " ('would', 'MD'),\n",
              " ('never', 'RB'),\n",
              " ('be', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('same.It', 'NN'),\n",
              " ('all', 'DT'),\n",
              " ('started', 'VBD'),\n",
              " ('with', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('random', 'JJ'),\n",
              " ('letter', 'NN'),\n",
              " ('.', '.'),\n",
              " ('Several', 'JJ'),\n",
              " ('of', 'IN'),\n",
              " ('those', 'DT'),\n",
              " ('were', 'VBD'),\n",
              " ('joined', 'JJ'),\n",
              " ('forces', 'NNS'),\n",
              " ('to', 'TO'),\n",
              " ('create', 'VB'),\n",
              " ('a', 'DT'),\n",
              " ('random', 'JJ'),\n",
              " ('word', 'NN'),\n",
              " ('.', '.'),\n",
              " ('The', 'DT'),\n",
              " ('words', 'NNS'),\n",
              " ('decided', 'VBD'),\n",
              " ('to', 'TO'),\n",
              " ('get', 'VB'),\n",
              " ('together', 'RB'),\n",
              " ('and', 'CC'),\n",
              " ('form', 'VB'),\n",
              " ('a', 'DT'),\n",
              " ('random', 'JJ'),\n",
              " ('sentence', 'NN'),\n",
              " ('.', '.'),\n",
              " ('They', 'PRP'),\n",
              " ('decided', 'VBD'),\n",
              " ('not', 'RB'),\n",
              " ('to', 'TO'),\n",
              " ('stop', 'VB'),\n",
              " ('there', 'RB'),\n",
              " ('and', 'CC'),\n",
              " ('it', 'PRP'),\n",
              " ('was', 'VBD'),\n",
              " (\"n't\", 'RB'),\n",
              " ('long', 'RB'),\n",
              " ('before', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('random', 'NN'),\n",
              " ('paragraph', 'NN'),\n",
              " ('had', 'VBD'),\n",
              " ('been', 'VBN'),\n",
              " ('cobbled', 'VBN'),\n",
              " ('together', 'RB'),\n",
              " ('.', '.'),\n",
              " ('The', 'DT'),\n",
              " ('question', 'NN'),\n",
              " ('was', 'VBD'),\n",
              " ('whether', 'IN'),\n",
              " ('or', 'CC'),\n",
              " ('not', 'RB'),\n",
              " ('they', 'PRP'),\n",
              " ('could', 'MD'),\n",
              " ('continue', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('momentum', 'NN'),\n",
              " ('long', 'RB'),\n",
              " ('enough', 'RB'),\n",
              " ('to', 'TO'),\n",
              " ('create', 'VB'),\n",
              " ('a', 'DT'),\n",
              " ('random', 'JJ'),\n",
              " ('short', 'JJ'),\n",
              " ('story.There', 'NN'),\n",
              " ('were', 'VBD'),\n",
              " (\"n't\", 'RB'),\n",
              " ('supposed', 'VBN'),\n",
              " ('to', 'TO'),\n",
              " ('be', 'VB'),\n",
              " ('dragons', 'NNS'),\n",
              " ('flying', 'VBG'),\n",
              " ('in', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('sky', 'NN'),\n",
              " ('.', '.'),\n",
              " ('First', 'NNP'),\n",
              " ('and', 'CC'),\n",
              " ('foremost', 'RB'),\n",
              " (',', ','),\n",
              " ('dragons', 'NNS'),\n",
              " ('did', 'VBD'),\n",
              " (\"n't\", 'RB'),\n",
              " ('exist', 'VB'),\n",
              " ('.', '.'),\n",
              " ('They', 'PRP'),\n",
              " ('were', 'VBD'),\n",
              " ('mythical', 'JJ'),\n",
              " ('creatures', 'NNS'),\n",
              " ('from', 'IN'),\n",
              " ('fantasy', 'JJ'),\n",
              " ('books', 'NNS'),\n",
              " ('like', 'IN'),\n",
              " ('unicorns', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('This', 'DT'),\n",
              " ('was', 'VBD'),\n",
              " ('something', 'NN'),\n",
              " ('that', 'IN'),\n",
              " ('Pete', 'NNP'),\n",
              " ('knew', 'VBD'),\n",
              " ('in', 'IN'),\n",
              " ('his', 'PRP$'),\n",
              " ('heart', 'NN'),\n",
              " ('to', 'TO'),\n",
              " ('be', 'VB'),\n",
              " ('true', 'JJ'),\n",
              " ('so', 'RB'),\n",
              " ('he', 'PRP'),\n",
              " ('was', 'VBD'),\n",
              " ('having', 'VBG'),\n",
              " ('a', 'DT'),\n",
              " ('difficult', 'JJ'),\n",
              " ('time', 'NN'),\n",
              " ('acknowledging', 'VBG'),\n",
              " ('that', 'IN'),\n",
              " ('there', 'EX'),\n",
              " ('were', 'VBD'),\n",
              " ('actually', 'RB'),\n",
              " ('fire-breathing', 'JJ'),\n",
              " ('dragons', 'NNS'),\n",
              " ('flying', 'VBG'),\n",
              " ('in', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('sky', 'NN'),\n",
              " ('above', 'IN'),\n",
              " ('him', 'PRP'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "t1bM6s-iYq3Y"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}